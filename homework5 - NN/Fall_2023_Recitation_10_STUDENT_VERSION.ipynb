{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Gsu_ssYQXK"
      },
      "source": [
        "# CIS 545 Recitation 10 - PyTorch and Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Loading"
      ],
      "metadata": {
        "id": "UDnD2RypG1Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "hnK-pUdTG2BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eef4c26-c0e4-4b2c-c1de-f0529960ca0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Brief Tutorial on PyTorch\n",
        "**Acknowledgement**: We appreciate the brief tutorial on PyTorch given by Prof. Jacob Gardner"
      ],
      "metadata": {
        "id": "-zs5RHAuFgR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making sure a GPU is available.\n",
        "\n",
        "Part of what we'll be doing in this tutorial is covering using GPUs for computation in PyTorch. In order to enable the use of a GPU in colab, you'll first need to go to `Runtime -> Change runtime type` in the menu system above. Then, under hardware acceleration, choose GPU.\n",
        "\n",
        "Once you've done that, the next cell should run without errors."
      ],
      "metadata": {
        "id": "vDsGRvVOEJBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running this cell shouldn't produce an error if you've done the above steps correctly.\n",
        "assert torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "Xx7hoaZCEMYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Tensors\n",
        "\n",
        "Tensors in PyTorch are direct equivalents of `ndarray` (or just arrays) in NumPy in many ways. In fact, many of the operations you are now familiar with in numpy translate directly over to PyTorch! Below are some examples."
      ],
      "metadata": {
        "id": "sQYjTSL5EQa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a 5x5 matrix, a 5x1 column vector, a 1x5 row vector and a (5,) \"proper\" vector in NumPy and PyTorch\n",
        "np_vec = np.arange(1, 6)  # (5,)\n",
        "np_rvec = np_vec[None, :]  # (1, 5)\n",
        "np_cvec = np_vec[:, None]  # (5, 1)\n",
        "np_mat = np.tile(np_rvec, (5, 1))  # Repeat the np_rvec row 5 times --> (5, 5)\n",
        "print(np_vec.shape, np_rvec.shape, np_cvec.shape, np_mat.shape)\n",
        "\n",
        "th_vec = torch.arange(1, 6)\n",
        "th_rvec = th_vec[None, :]  # Or: th_vec.unsqueeze(0)\n",
        "th_cvec = th_vec[:, None]  # Or: th_vec.unsqueeze(1)\n",
        "th_mat = torch.tile(th_rvec, (5, 1))\n",
        "print(th_vec.shape, th_rvec.shape, th_cvec.shape, th_mat.shape)\n",
        "\n",
        "print('NumPy matrix...')\n",
        "print(np_mat)\n",
        "print('Torch matrix...')\n",
        "print(th_mat)"
      ],
      "metadata": {
        "id": "r3JMGzNREOZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e365bc-f0e4-472e-92cb-9c7b4891edbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5,) (1, 5) (5, 1) (5, 5)\n",
            "torch.Size([5]) torch.Size([1, 5]) torch.Size([5, 1]) torch.Size([5, 5])\n",
            "NumPy matrix...\n",
            "[[1 2 3 4 5]\n",
            " [1 2 3 4 5]\n",
            " [1 2 3 4 5]\n",
            " [1 2 3 4 5]\n",
            " [1 2 3 4 5]]\n",
            "Torch matrix...\n",
            "tensor([[1, 2, 3, 4, 5],\n",
            "        [1, 2, 3, 4, 5],\n",
            "        [1, 2, 3, 4, 5],\n",
            "        [1, 2, 3, 4, 5],\n",
            "        [1, 2, 3, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As a reminder, here are some operations you can do in NumPy on matrices and vectors.\n",
        "\n",
        "# Matrix vector multiplication\n",
        "np_mvm = np_mat @ np_cvec  # or np.matmul\n",
        "\n",
        "# Add np_rvec to each row of np_mat\n",
        "np_add1 = np_mat + np_rvec\n",
        "\n",
        "# Subtract np_cvec from each column of np_mat\n",
        "np_add2 = np_mat - np_cvec\n",
        "\n",
        "# Take the square root of each element of np_mat\n",
        "np_sqrt = np.sqrt(np_mat)"
      ],
      "metadata": {
        "id": "DH1q8fyCEaI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_mvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DASw6pzWtWQl",
        "outputId": "569a61cb-fe64-4b71-d8ee-b898c5a82c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[55],\n",
              "       [55],\n",
              "       [55],\n",
              "       [55],\n",
              "       [55]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replicating the above operations on the PyTorch operations.\n",
        "\n",
        "# Matrix vector multiplication\n",
        "th_mvm = th_mat @ th_cvec  # or torch.matmul\n",
        "\n",
        "# Add np_rvec to each row of np_mat\n",
        "th_add1 = th_mat + th_rvec\n",
        "\n",
        "# Subtract np_cvec from each column of np_mat\n",
        "th_add2 = th_mat - th_cvec\n",
        "\n",
        "# Take the square root of each element of np_mat\n",
        "th_sqrt = torch.sqrt(th_mat)"
      ],
      "metadata": {
        "id": "JDfjc4zKEcvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_mvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quUmmoyftX9j",
        "outputId": "c1848930-912c-4b76-a53e-12f96c84d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[55],\n",
              "        [55],\n",
              "        [55],\n",
              "        [55],\n",
              "        [55]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Difference 1: GPU Computing\n",
        "\n",
        "The first major difference between `NumPy` and `PyTorch` is that PyTorch supports easy use of GPUs for computation. The way this works is as follows:\n",
        "\n",
        "1. First, move any tensors that you want to do computation on the GPU with to the GPU. This can be accomplished by calling either `gpu_tensor = tensor.cuda()` or `gpu_tensor = tensor.to('cuda')`. If you have more than one GPU on your computer, you can also specify which GPU to use, e.g. `tensor.to('cuda:1')`.\n",
        "\n",
        "**Note**: this will fail if you don't have a GPU available.\n",
        "\n",
        "2. There is no step 2! Do computation with the tensors as normal and it all happens on the GPU."
      ],
      "metadata": {
        "id": "j38zL5K3EfGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a random 5000x5000 matrix in torch and do a matrix multiply with a 1000x1000 matrix of all ones.\n",
        "# Hint: use torch.rand(n, m) and torch.ones(n, m)\n",
        "\n",
        "th_mat1 = torch.rand(5000, 5000)\n",
        "th_mat2 = torch.ones(5000, 5000)"
      ],
      "metadata": {
        "id": "VYTBghfdEhbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Timing a matrix multiply on the CPU\n",
        "\n",
        "%time res = th_mat1 @ th_mat2"
      ],
      "metadata": {
        "id": "1Ui2zPETEjiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cebb902-32e6-4601-e418-cfeca81dbee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.88 s, sys: 69.8 ms, total: 2.95 s\n",
            "Wall time: 6.49 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making new tensors Move th_mat1 and th_mat2 to the GPU\n",
        "th_mat1_gpu = th_mat1.cuda()\n",
        "th_mat2_gpu = th_mat2.cuda()"
      ],
      "metadata": {
        "id": "ZSRwh_CLElYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Filling in \"res\" below.\n",
        "\n",
        "res = th_mat1_gpu @ th_mat2_gpu\n",
        "\n",
        "print(res[0, 0])"
      ],
      "metadata": {
        "id": "Cfc5p28VEoCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecf9643-ed5e-40af-f157-911fe9926dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2462.1714, device='cuda:0')\n",
            "CPU times: user 465 ms, sys: 247 ms, total: 711 ms\n",
            "Wall time: 2.71 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Difference 2: Autograd\n",
        "\n",
        "The second major difference between `NumPy` and `PyTorch` is that PyTorch supports automatic differentiation. What this means is that PyTorch allows you to do computation and get derivatives for free! Here's the basic work flow:\n",
        "\n",
        "1. Define a Parameter with `some_param = torch.nn.Parameter(some_tensor)`.\n",
        "1. Use the parameter in some computation.\n",
        "1. Call `.backward()` on any scalar result of the computation to get derivatives for some_param in `some_param.grad`"
      ],
      "metadata": {
        "id": "bHgE61P4Erw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_tensor = torch.rand(1)\n",
        "# Making a parameter out of some_param\n",
        "some_param = torch.nn.Parameter(some_tensor)\n",
        "\n",
        "# Computing sin(exp(some_param)) and the derivative of sin(exp(x)) with respect to some_param.\n",
        "# Hint: Use torch.sin and torch.exp\n",
        "res = torch.sin(torch.exp(some_param))\n",
        "res.backward()\n",
        "\n",
        "print(res, some_param.grad)"
      ],
      "metadata": {
        "id": "6B0nBSMXExcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5129ac-4215-4343-a140-20bcd23c2cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4899], grad_fn=<SinBackward0>) tensor([-2.2926])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, autograd works for multivariate calculus, too. Let's compute the partial derivatives of the following 5 dimensional function: $$f(\\mathbf{x}) = \\sum_{i=1}^{5} \\sin ( \\exp (x_i) )$$ at a few inputs. E.g., we're going to compute $\\frac{\\partial f}{\\partial x_i}$ for all i."
      ],
      "metadata": {
        "id": "bxz8O8AjE0ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a random parameter of length 5.\n",
        "param = torch.nn.Parameter(torch.rand(5,))\n",
        "res = torch.sum(torch.sin(torch.exp(param)))\n",
        "res.backward()\n",
        "\n",
        "print(res, param.grad)"
      ],
      "metadata": {
        "id": "8zDZfMK4E1el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc8630e2-676e-491d-ec18-2705a324bec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.4817, grad_fn=<SumBackward0>) tensor([ 0.1638, -1.0823, -0.2724,  0.3176, -1.7836])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Block of ML #1: torch.nn.Module\n",
        "\n",
        "In PyTorch, we can define Modules. Modules do two things for me:\n",
        "\n",
        "1. They make it easy to collect a set of parameters together. By calling `module.parameters()` or `module.named_parameters()` I get a generator over the parameters not only of the module but all of its submodules.\n",
        "2. They let me define a method called `forward` that gets called when I do `module(x)`.\n",
        "\n",
        "Let's first code up a \"from scratch\" implementation of a module that might be useful for linear regression. To do this, we need to create a class that extends `Module`, defines `weight` and `bias` parameters in the constructor, and then defines a `forward` method that applies the familiar linear regression prediction equation, $w^{\\top}x + b$"
      ],
      "metadata": {
        "id": "_vokwSPkFXIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: The number of features we expect in the dataset.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Adding a \"weight\" and \"bias\" parameter to this class.\n",
        "        self.weight = torch.nn.Parameter(torch.rand(input_dim, 1))\n",
        "        self.bias = torch.nn.Parameter(torch.rand(1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assume: x is (n, d)\n",
        "        # Computing linear regression predictions for each x using the parameters you define above.\n",
        "        prediction = (x @ self.weight) + self.bias\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "6hu3Iw5gFtEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load data ==> in NumPy array\n",
        "X, y = load_diabetes(return_X_y=True)\n",
        "# split into test and train set\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# move from NumPy to be in Torch array\n",
        "train_x, train_y = torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float()\n",
        "test_x, test_y = torch.from_numpy(test_x).float(), torch.from_numpy(test_y).float()\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "Zptirp4mFuX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eefcda1-ebc3-406c-c38c-f9933a7670f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([296, 10]) torch.Size([296]) torch.Size([146, 10]) torch.Size([146])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a new LinearRegression object with appropriate input_dim for the above dataset.\n",
        "# Then, making predictions on train_x, and computing the derivative of the MSE with respect to your weight and bias.\n",
        "\n",
        "linear_reg = LinearRegression(input_dim=10)\n",
        "pred = linear_reg(train_x)\n",
        "mse = torch.mean((pred - train_y) ** 2)\n",
        "mse.backward()\n",
        "print(linear_reg.weight.grad, linear_reg.bias.grad)"
      ],
      "metadata": {
        "id": "b_cPLZ_hGAue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9bb96e-62f3-459d-afbe-c4cb8640f39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8927],\n",
            "        [-0.3269],\n",
            "        [-0.7421],\n",
            "        [-0.5722],\n",
            "        [-0.2978],\n",
            "        [-0.2365],\n",
            "        [ 0.3175],\n",
            "        [-0.4559],\n",
            "        [-0.5916],\n",
            "        [-0.5155]]) tensor([[-305.3282]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Block of ML #2: torch.optim\n",
        "\n",
        "We're clearly making progress! So far, what we've been able to do is define a linear regression module with appropriate parameters, compute a loss function (mean squared error), and compute the derivative of the loss with respect to the weight and bias. All we need now is a way to apply these gradients to the parameters to update them and learn.\n",
        "\n",
        "To help accomplish this, PyTorch provides a `torch.optim` library with a variety of `Optimizers`. In PyTorch, an `Optimizer` has the following properties:\n",
        "\n",
        "1. You create an optimizer with a list (or generator) of parameters to optimize, and typically a learning rate / step size.\n",
        "1. Optimizers expose a `zero_grad()` method that resets the gradients of all parameters to zero.\n",
        "1. Optimizers expose a `step()` method that, if all parameters have had gradients filled in, applies a step of optimization with those gradients.\n",
        "\n",
        "Below, we'll be using one optimizer called Adam, which is provided via `torch.optim.Adam`."
      ],
      "metadata": {
        "id": "zNPfPalsGIqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a torch.optim.Adam object to take a single step of learning on the MSE of our linear regressor above.\n",
        "# Hint: Creating an Adam object can be done with torch.optim.Adam(some_parameters, lr=0.01)\n",
        "# An optimization loop should: (1) zero the gradients, (2) compute the loss, (3) call backward, (4) call step.\n",
        "\n",
        "opt = torch.optim.Adam(linear_reg.parameters(), lr=0.01)\n",
        "opt.zero_grad()\n",
        "pred = linear_reg(train_x)\n",
        "loss = torch.mean((pred - train_y) ** 2)\n",
        "loss.backward()\n",
        "opt.step()"
      ],
      "metadata": {
        "id": "0bSbb1A6GL0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the linear regression model for 2500 iterations, and print out the loss every 100 iterations.\n",
        "\n",
        "linear_reg = LinearRegression(input_dim=10)\n",
        "opt = torch.optim.Adam(linear_reg.parameters(), lr=0.1)\n",
        "\n",
        "for i in range(2500):\n",
        "    opt.zero_grad()\n",
        "    pred = linear_reg(train_x)\n",
        "    loss = torch.mean((pred.squeeze() - train_y) ** 2)\n",
        "    if i % 100 == 0:\n",
        "        print(f'Iteration {i} - Loss = {loss:.2f}')\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "metadata": {
        "id": "qKpHMr8_GMwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2077c979-4ac7-45b9-97e1-f68844bf6e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Loss = 29331.09\n",
            "Iteration 100 - Loss = 26116.49\n",
            "Iteration 200 - Loss = 23216.23\n",
            "Iteration 300 - Loss = 20611.75\n",
            "Iteration 400 - Loss = 18281.99\n",
            "Iteration 500 - Loss = 16206.61\n",
            "Iteration 600 - Loss = 14365.91\n",
            "Iteration 700 - Loss = 12740.88\n",
            "Iteration 800 - Loss = 11313.17\n",
            "Iteration 900 - Loss = 10065.11\n",
            "Iteration 1000 - Loss = 8979.85\n",
            "Iteration 1100 - Loss = 8041.38\n",
            "Iteration 1200 - Loss = 7234.57\n",
            "Iteration 1300 - Loss = 6545.26\n",
            "Iteration 1400 - Loss = 5960.17\n",
            "Iteration 1500 - Loss = 5466.92\n",
            "Iteration 1600 - Loss = 5053.97\n",
            "Iteration 1700 - Loss = 4710.60\n",
            "Iteration 1800 - Loss = 4426.96\n",
            "Iteration 1900 - Loss = 4194.00\n",
            "Iteration 2000 - Loss = 4003.58\n",
            "Iteration 2100 - Loss = 3848.39\n",
            "Iteration 2200 - Loss = 3722.03\n",
            "Iteration 2300 - Loss = 3618.95\n",
            "Iteration 2400 - Loss = 3534.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing test predictions and test MSE\n",
        "pred = linear_reg(test_x)\n",
        "test_error = torch.mean((pred.squeeze() - test_y) ** 2)\n",
        "print(test_error)"
      ],
      "metadata": {
        "id": "tqZHNd1QGahq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e500e4-675a-49a8-e7a0-7d5d75e86082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3599.8232, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.nn: A convenient library of pre baked modules.\n",
        "\n",
        "torch.nn contains much more than just `Parameter` and `Module`: it contains a variety of pre baked modules that are useful for machine learning, including:\n",
        "- `torch.nn.Linear`: Has weight and bias parameters, and applies an affine transformation $XW + b$ to an input.\n",
        "- `torch.nn.ReLU`: Applies the rectified linear unit (ReLU) to the input.\n",
        "- `torch.nn.Conv2d`: Applies 2D convolutions.\n",
        "- `torch.nn.MaxPool2d`: Applies max pooling.\n",
        "\n",
        "And many more! Let's use these to build a simple 1 hidden layer neural network on the boston data above."
      ],
      "metadata": {
        "id": "8hZKVa1XGiHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeating the above linear regression, but using a torch.nn.Linear module instead of our \"from scratch\" version.\n",
        "\n",
        "linear_reg = torch.nn.Linear(in_features=10, out_features=1)\n",
        "opt = torch.optim.Adam(linear_reg.parameters(), lr=0.1)\n",
        "\n",
        "for i in range(2500):\n",
        "    opt.zero_grad()\n",
        "    pred = linear_reg(train_x)\n",
        "    loss = torch.mean((pred.squeeze() - train_y) ** 2)\n",
        "    if i % 100 == 0:\n",
        "        print(f'Iteration {i} - Loss = {loss:.2f}')\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "metadata": {
        "id": "1LUCnO7uGkPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e759690-7590-4c17-fc31-1d5af4d8d02f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Loss = 29484.99\n",
            "Iteration 100 - Loss = 26260.17\n",
            "Iteration 200 - Loss = 23349.74\n",
            "Iteration 300 - Loss = 20735.19\n",
            "Iteration 400 - Loss = 18395.54\n",
            "Iteration 500 - Loss = 16310.48\n",
            "Iteration 600 - Loss = 14460.41\n",
            "Iteration 700 - Loss = 12826.33\n",
            "Iteration 800 - Loss = 11389.96\n",
            "Iteration 900 - Loss = 10133.68\n",
            "Iteration 1000 - Loss = 9040.67\n",
            "Iteration 1100 - Loss = 8094.94\n",
            "Iteration 1200 - Loss = 7281.39\n",
            "Iteration 1300 - Loss = 6585.86\n",
            "Iteration 1400 - Loss = 5995.10\n",
            "Iteration 1500 - Loss = 5496.70\n",
            "Iteration 1600 - Loss = 5079.14\n",
            "Iteration 1700 - Loss = 4731.69\n",
            "Iteration 1800 - Loss = 4444.46\n",
            "Iteration 1900 - Loss = 4208.42\n",
            "Iteration 2000 - Loss = 4015.36\n",
            "Iteration 2100 - Loss = 3857.96\n",
            "Iteration 2200 - Loss = 3729.78\n",
            "Iteration 2300 - Loss = 3625.23\n",
            "Iteration 2400 - Loss = 3539.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing test predictions and test MSE\n",
        "pred = linear_reg(test_x)\n",
        "test_error = torch.mean((pred.squeeze() - test_y) ** 2)\n",
        "print(test_error)"
      ],
      "metadata": {
        "id": "cPUjpzavGmom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87ddbbb-b267-4be0-ad18-77a6c08884f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3604.5852, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a feed forward neural network Module with one hidden layer.\n",
        "\n",
        "class NeuralNet(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # First layer: map from input to hidden features.\n",
        "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Second layer: map from hidden dim to prediction size (1)\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_feat = self.linear1(x)\n",
        "        hidden_feat = torch.relu(hidden_feat)\n",
        "        prediction = self.linear2(hidden_feat)\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "_-37-bw_GqyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a neural network with a hidden dimensionality of 64.\n",
        "# Training the neural network using Adam for 5000 iterations using a learning rate of 0.01.\n",
        "# Printing the loss every 100 iterations.\n",
        "\n",
        "neural_net = NeuralNet(input_dim=10, hidden_dim=64)\n",
        "opt = torch.optim.Adam(neural_net.parameters(), lr=0.01)\n",
        "\n",
        "for i in range(5000):\n",
        "    opt.zero_grad()\n",
        "    pred = neural_net(train_x)\n",
        "    loss = torch.mean((pred.squeeze() - train_y) ** 2)\n",
        "    if i % 100 == 0:\n",
        "        print(f'Iteration {i} - Loss = {loss:.2f}')\n",
        "    loss.backward()\n",
        "    opt.step()"
      ],
      "metadata": {
        "id": "7Lck5Pw_GuW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5af7aa-ab95-45d3-8c11-6d4dfa68f479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 - Loss = 29460.39\n",
            "Iteration 100 - Loss = 13482.79\n",
            "Iteration 200 - Loss = 3622.69\n",
            "Iteration 300 - Loss = 3224.86\n",
            "Iteration 400 - Loss = 3044.63\n",
            "Iteration 500 - Loss = 2965.34\n",
            "Iteration 600 - Loss = 2929.07\n",
            "Iteration 700 - Loss = 2910.57\n",
            "Iteration 800 - Loss = 2900.31\n",
            "Iteration 900 - Loss = 2894.39\n",
            "Iteration 1000 - Loss = 2890.85\n",
            "Iteration 1100 - Loss = 2888.62\n",
            "Iteration 1200 - Loss = 2887.12\n",
            "Iteration 1300 - Loss = 2886.01\n",
            "Iteration 1400 - Loss = 2885.12\n",
            "Iteration 1500 - Loss = 2884.34\n",
            "Iteration 1600 - Loss = 2883.62\n",
            "Iteration 1700 - Loss = 2882.92\n",
            "Iteration 1800 - Loss = 2882.24\n",
            "Iteration 1900 - Loss = 2881.56\n",
            "Iteration 2000 - Loss = 2880.88\n",
            "Iteration 2100 - Loss = 2880.21\n",
            "Iteration 2200 - Loss = 2879.54\n",
            "Iteration 2300 - Loss = 2878.87\n",
            "Iteration 2400 - Loss = 2878.20\n",
            "Iteration 2500 - Loss = 2877.55\n",
            "Iteration 2600 - Loss = 2876.90\n",
            "Iteration 2700 - Loss = 2876.26\n",
            "Iteration 2800 - Loss = 2875.64\n",
            "Iteration 2900 - Loss = 2875.03\n",
            "Iteration 3000 - Loss = 2874.44\n",
            "Iteration 3100 - Loss = 2873.86\n",
            "Iteration 3200 - Loss = 2873.31\n",
            "Iteration 3300 - Loss = 2872.79\n",
            "Iteration 3400 - Loss = 2872.28\n",
            "Iteration 3500 - Loss = 2871.81\n",
            "Iteration 3600 - Loss = 2871.36\n",
            "Iteration 3700 - Loss = 2870.95\n",
            "Iteration 3800 - Loss = 2870.56\n",
            "Iteration 3900 - Loss = 2870.21\n",
            "Iteration 4000 - Loss = 2869.88\n",
            "Iteration 4100 - Loss = 2869.59\n",
            "Iteration 4200 - Loss = 2869.33\n",
            "Iteration 4300 - Loss = 2869.10\n",
            "Iteration 4400 - Loss = 2868.90\n",
            "Iteration 4500 - Loss = 2868.73\n",
            "Iteration 4600 - Loss = 2868.58\n",
            "Iteration 4700 - Loss = 2868.45\n",
            "Iteration 4800 - Loss = 2868.35\n",
            "Iteration 4900 - Loss = 2868.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing test predictions and testing MSE\n",
        "pred = neural_net(test_x)\n",
        "test_error = torch.mean((pred.squeeze() - test_y) ** 2)\n",
        "print(test_error)+"
      ],
      "metadata": {
        "id": "Cmwzi7-qGwYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "e1b1fcda-5914-46a8-b380-766e5e6f8631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-374d5317db88>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(test_error)+\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls6hAlPGbai5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Convolutional Neural Networks (CNN)\n",
        "\n",
        "#### Here we are using the PyTorch library to get the dataset and make a dataloader.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTH9t9_gdybL"
      },
      "source": [
        "More details on the CIFAR-10 dataset as well as training a CNN model can be found [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
        "\n",
        "<img src='https://pytorch.org/tutorials/_images/cifar10.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform\n",
        "\n",
        "#### Here we need to define a series of transformations we want to perform on our dataset before we feed it to the Neural Network."
      ],
      "metadata": {
        "id": "G-G5bPi_crIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transformations applied to the images\n",
        "transformations = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "cUKEWcadn2HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1xIADSfVzIV"
      },
      "source": [
        "# TODO:\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data', train=True, download=True, transform = transformations)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data', train=False, download=True, transform = transformations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a dataloader. For Train / Test"
      ],
      "metadata": {
        "id": "9Cm8-do4dPm4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1DMyS2DV1tc"
      },
      "source": [
        "# TODO:\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Image Visualization"
      ],
      "metadata": {
        "id": "o799cI36dXNd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8nEGYPtV3pc"
      },
      "source": [
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "# TODO: Get some random training images\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040YkbEDp4y8"
      },
      "source": [
        "\n",
        "### A Basic CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtA2ULbOBliT"
      },
      "source": [
        "Check out [here](https://madebyollin.github.io/convnet-calculator/) for a helper calculator you can use for the layer dimensions!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some basic checks..."
      ],
      "metadata": {
        "id": "j3YKOzzOdh1a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu6n9ybDiCiX"
      },
      "source": [
        "for inputs, labels in train_loader:\n",
        "    print(\"The shape of inputs is:\", inputs.shape)\n",
        "    print(\"The shape of labels is:\", labels.shape)\n",
        "    break\n",
        "\n",
        "print(\"Number of classes:\", len(train_dataset.classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining CNN Architecture"
      ],
      "metadata": {
        "id": "bLh2JWUVdwQf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiBQRkpap4a8"
      },
      "source": [
        "# TODO: Define a CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=4, stride=2)\n",
        "        # self.conv2 = nn.Conv2d(in_channels=3, out_channels=40, kernel_size=4, stride=2)\n",
        "        self.mp = nn.MaxPool2d(kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        self.fc = nn.Linear(in_features=20*4*4, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.conv(x) # changes dimension here\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.mp(outputs) # changes dimension here ==> 16,20,5,5\n",
        "        outputs = self.flatten(outputs) # 16, 20*5*5\n",
        "        outputs = self.fc(outputs) # 16, 10\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the CNN"
      ],
      "metadata": {
        "id": "68lnb7k0d2qZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if GPU is available and setting up the device variable"
      ],
      "metadata": {
        "id": "JpB0uLVIIt8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "XLULbONBIsUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73ffd48-04eb-4465-ba56-5e84f12f3aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP664uQDV8j-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f19f119-d2f8-401f-e373-5e5dbc615321"
      },
      "source": [
        "## TODO:\n",
        "# Sending the data to device (CPU or GPU)\n",
        "cnn = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss() # equivalent to applying LogSoftmax() to output and then use NLLLoss()\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=1e-4) #lr - learning step\n",
        "\n",
        "loss_LIST = []\n",
        "\n",
        "# Epochs 3\n",
        "for epoch in range(3):\n",
        "  running_loss = 0.0\n",
        "  for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device) # Send the inputs and labels to the device\n",
        "      outputs = cnn(inputs) # Feed the network the train data\n",
        "      optimizer.zero_grad() # We need to reset the optimizer tensor gradient every mini-batch\n",
        "      loss = criterion(outputs, labels) # this is the average loss for one mini-batch of inputs\n",
        "      loss.backward() # Do a back propagation\n",
        "      optimizer.step() # Update the weight using the gradients from back propagation by learning step\n",
        "\n",
        "      running_loss += loss.item() #get the accumulated loss for each epoch\n",
        "  loss_LIST.append(running_loss / len(train_loader)) # get the avg loss for each epoch\n",
        "\n",
        "  # print statistics\n",
        "  print(f'The loss for Epoch {epoch} is: {running_loss/len(train_loader)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss for Epoch 0 is: 1.9798614526367186\n",
            "The loss for Epoch 1 is: 1.6965392213058472\n",
            "The loss for Epoch 2 is: 1.6003207152175902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the Accuracy"
      ],
      "metadata": {
        "id": "sGbr7eP2fGzq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVbrpBM9V_Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3ab0d4-45dc-4e7d-a7b5-3eb622b536f2"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device) # Send the inputs and labels to the device\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1) # use max to get the prediction\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100 * correct / total}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 46.25\n"
          ]
        }
      ]
    }
  ]
}